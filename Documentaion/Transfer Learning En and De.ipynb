{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'dataA', 'dataB', 'dataE', 'dataD', 'dataC']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "\n",
    "from tensorflow.python.framework.random_seed import set_random_seed\n",
    "set_random_seed(123)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import imageio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"/Users/nouf/Desktop/archive\"))\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = [\"/Users/nouf/Desktop/archive/\"+\"data\"+i+\"/\"+\"data\"+i+\"/CameraRGB/\" for i in ['A', 'B', 'C', 'D', 'E']]\n",
    "mask_path = [\"/Users/nouf/Desktop/archive/\"+\"data\"+i+\"/\"+\"data\"+i+\"/CameraSeg/\" for i in ['A', 'B', 'C', 'D', 'E']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_image_paths(directory_paths):\n",
    "    image_paths = []\n",
    "    for directory in range(len(directory_paths)):\n",
    "        image_filenames = os.listdir(directory_paths[directory])\n",
    "        for image_filename in image_filenames:\n",
    "            image_paths.append(directory_paths[directory] + image_filename)\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. There are 5000 images and 5000 masks in our dataset\n",
      "2. An example of an image path is: \n",
      " /Users/nouf/Desktop/archive/dataA/dataA/CameraRGB/F7-31.png\n",
      "3. An example of a mask path is: \n",
      " /Users/nouf/Desktop/archive/dataA/dataA/CameraSeg/F7-31.png\n"
     ]
    }
   ],
   "source": [
    "image_paths = list_image_paths(image_path) \n",
    "mask_paths = list_image_paths(mask_path)\n",
    "number_of_images, number_of_masks = len(image_paths), len(mask_paths)\n",
    "print(f\"1. There are {number_of_images} images and {number_of_masks} masks in our dataset\")\n",
    "print(f\"2. An example of an image path is: \\n {image_paths[0]}\")\n",
    "print(f\"3. An example of a mask path is: \\n {mask_paths[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = '/Users/nouf/Desktop/archive/dataA/dataA/CameraRGB/'\n",
    "# mask_path = '/Users/nouf/Desktop/archive/dataA/dataA/CameraSeg/'\n",
    "# image_list = os.listdir(image_path)\n",
    "# mask_list = os.listdir(mask_path)\n",
    "# image_list = [image_path+i for i in image_list]\n",
    "# mask_list = [mask_path+i for i in mask_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = image_paths\n",
    "Y = mask_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_imageMask(image_path, mask_path):\n",
    "    \n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, (128, 128), method='nearest')\n",
    "    \n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=3)\n",
    "    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n",
    "    mask = tf.image.resize(mask, (128, 128), method='nearest')\n",
    "    \n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(image_paths, mask_paths, batch_size):\n",
    "    images = tf.constant(image_paths)\n",
    "    masks = tf.constant(mask_paths)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, masks))\n",
    "    dataset = dataset.map(read_imageMask, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.cache().shuffle(500).batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_generator(X_train, Y_train, 32)\n",
    "val_generator = data_generator(X_val, Y_val, 32)\n",
    "test_generator = data_generator(X_test, Y_test, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
    "\n",
    "# Use the activations of these layers\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',   # 64x64\n",
    "    'block_3_expand_relu',   # 32x32\n",
    "    'block_6_expand_relu',   # 16x16\n",
    "    'block_13_expand_relu',  # 8x8\n",
    "    'block_16_project',      # 4x4\n",
    "]\n",
    "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Create the feature extraction model\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_examples.models.pix2pix import pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(output_channels:int):\n",
    "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = down_stack(inputs)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  # This is the last layer of the model\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=output_channels, kernel_size=3, strides=2,\n",
    "      padding='same')  #64x64 -> 128x128\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CLASSES = 13\n",
    "\n",
    "model = unet_model(output_channels=OUTPUT_CLASSES)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor='val_loss', patience= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6720 - accuracy: 0.8203"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, validation_data=val_generator,epochs=50,\n",
    "                    verbose = 1, batch_size=16,\n",
    "                    callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_accuracy = model.evaluate(train_generator, batch_size = 32)\n",
    "validation_loss, validation_accuracy = model.evaluate(val_generator, batch_size = 32)\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for image, mask in test_generator.take(6):\n",
    "    pred_mask = model.predict(image)\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(image[0]))\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(mask[0]))\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(tf.expand_dims(tf.argmax(pred_mask[0], axis = -1), axis = -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(dataset, model):\n",
    "    true_masks, predicted_masks = [], []\n",
    "    for images, masks in dataset:\n",
    "        pred_masks = model.predict(images)\n",
    "        pred_masks = tf.expand_dims(tf.argmax(pred_masks, axis=-1), axis=-1)\n",
    "        true_masks.extend(masks)\n",
    "        predicted_masks.extend(pred_masks)\n",
    "        \n",
    "    true_masks = np.array(true_masks)\n",
    "    predicted_masks = np.array(predicted_masks)\n",
    "        \n",
    "    return true_masks, predicted_masks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_train_masks, predicted_train_masks = create_mask(train_generator, model)\n",
    "true_validation_masks, predicted_validation_masks = create_mask(val_generator, model)\n",
    "true_test_masks, predicted_test_masks = create_mask(test_generator, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true_masks, predicted_masks, n_classes, smooth = 1e-6):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluates semantic segmentation model\n",
    "    \n",
    "    Argument:\n",
    "        true_masks: ground truth segmentations\n",
    "        predicted_masks: predicted segmentations\n",
    "        n_classes: number of segmentation classes\n",
    "        smooth: a minute float digit added to denominators to avoid error from a zero division\n",
    "    \n",
    "    Returns:\n",
    "        class_wise_evaluations: a dictionary containing evaluation metric \n",
    "                                outputs the for each segmentation class \n",
    "        overall_evaluations: a dictionary containing evaluation metric \n",
    "                             outputs the for all segmentation classes\n",
    "        \"\"\"\n",
    "    # Create empty lists to store evaluation metric outputs\n",
    "    class_wise_true_positives, class_wise_true_negatives = [],[]\n",
    "    class_wise_false_positives, class_wise_false_negatives = [],[]\n",
    "    class_wise_precisions, class_wise_recalls = [],[] \n",
    "    class_wise_specificities, class_wise_ious = [],[] \n",
    "    class_wise_tdrs, class_wise_f1_scores = [],[]\n",
    "    classes = []\n",
    "            \n",
    "    for clas in range(n_classes):\n",
    "        true_positives, true_negatives, false_positives, false_negatives = 0,0,0,0\n",
    "        precisions, recalls, specificities, ious, f1_scores, tdrs = 0,0,0,0,0,0        \n",
    "        \n",
    "        number_of_masks = true_masks.shape[0]\n",
    "        \n",
    "        for mask_id in range(number_of_masks):\n",
    "            true_positive = np.sum(np.logical_and(true_masks[mask_id]==clas, predicted_masks[mask_id]==clas))\n",
    "            true_negative = np.sum(np.logical_and(true_masks[mask_id]!=clas, predicted_masks[mask_id]!=clas))\n",
    "            false_positive = np.sum(np.logical_and(true_masks[mask_id]!=clas, predicted_masks[mask_id]==clas))\n",
    "            false_negative = np.sum(np.logical_and(true_masks[mask_id]==clas, predicted_masks[mask_id]!=clas))\n",
    "            \n",
    "            true_positives += true_positive\n",
    "            true_negatives += true_negative\n",
    "            false_positives += false_positive\n",
    "            false_negatives += false_negative\n",
    "\n",
    "        recall = round(true_positives/(true_positives + false_negatives + smooth), 2)\n",
    "        precision = round(true_positives/(true_positives + false_positives + smooth), 2)\n",
    "        specificity = round(true_negatives/(true_negatives + false_positives + smooth), 2)\n",
    "        tdr = round((1 - (false_negatives/(true_positives + false_negatives + smooth))), 2)\n",
    "        iou = round(true_positives/(true_positives + false_negatives + false_positives + smooth), 2)\n",
    "        f1_score = round((2 * precision * recall)/(precision + recall + smooth), 2)\n",
    "        \n",
    "        class_wise_true_positives.append(true_positives)\n",
    "        class_wise_true_negatives.append(true_negatives)\n",
    "        class_wise_false_positives.append(false_positives)\n",
    "        class_wise_false_negatives.append(false_negatives)\n",
    "        class_wise_recalls.append(recall)\n",
    "        class_wise_precisions.append(precision)\n",
    "        class_wise_specificities.append(specificity)\n",
    "        class_wise_ious.append(iou)\n",
    "        class_wise_tdrs.append(tdr)\n",
    "        class_wise_f1_scores.append(f1_score)\n",
    "        classes.append(\"Class \" + str(clas+1))\n",
    "        # class_wise_pixel_accuracies.append(pixel_accuracy)\n",
    "        \n",
    "    total_true_positives = np.sum(class_wise_true_positives)\n",
    "    total_true_negatives = np.sum(class_wise_true_negatives)\n",
    "    total_false_positives = np.sum(class_wise_false_positives)\n",
    "    total_false_negatives = np.sum(class_wise_false_negatives)\n",
    "    mean_recall = round(np.average(np.array(class_wise_recalls)), 2)\n",
    "    mean_precision = round(np.average(np.array(class_wise_precisions)), 2)\n",
    "    mean_specificity = round(np.average(np.array(class_wise_specificities)), 2)\n",
    "    mean_iou = round(np.average(np.array(class_wise_ious)), 2)\n",
    "    mean_tdr = round(np.average(np.array(class_wise_tdrs)), 2)\n",
    "    mean_f1_score = round(np.average(np.array(class_wise_f1_scores)), 2)    \n",
    "         \n",
    "    class_wise_evaluations = {\"Class\": classes,\n",
    "                              \"True Positive Pixels\": class_wise_true_positives,\n",
    "                              \"True Negative Pixels\": class_wise_true_negatives,\n",
    "                              \"False Positive Pixels\": class_wise_false_positives,\n",
    "                              \"False Negative Pixels\": class_wise_false_negatives,\n",
    "                              \"Recall\": class_wise_recalls,\n",
    "                              \"Precision\": class_wise_precisions,\n",
    "                              \"Specificity\": class_wise_specificities,\n",
    "                              \"IoU\": class_wise_ious,\n",
    "                              \"TDR\": class_wise_tdrs,\n",
    "                              \"F1-Score\": class_wise_f1_scores}\n",
    "\n",
    "    overall_evaluations = {\"Class\": \"All Classes\",\n",
    "                        \"True Positive Pixels\": total_true_positives,\n",
    "                        \"True Negative Pixels\": total_true_negatives,\n",
    "                        \"False Positive Pixels\": total_false_positives,\n",
    "                        \"False Negative Pixels\": total_false_negatives,\n",
    "                        \"Recall\": mean_recall,\n",
    "                        \"Precision\": mean_precision,\n",
    "                        \"Specificity\": mean_specificity,\n",
    "                        \"IoU\": mean_iou,\n",
    "                        \"TDR\": mean_tdr,\n",
    "                        \"F1-Score\": mean_f1_score}\n",
    "    \n",
    "    evaluations = {\"Overall Evaluations\": overall_evaluations, \n",
    "                   \"Class-wise Evaluations\": class_wise_evaluations}\n",
    "    \n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_evaluations(evaluations, \n",
    "                     metrics=[\"Recall\", \"Precision\", \"Specificity\", \"IoU\", \"TDR\", \"F1 Score\"], \n",
    "                     class_list=None,\n",
    "                     display_evaluations=\"All\"):\n",
    "    \"\"\"\n",
    "    Returns a pandas dataframe containing specified metrics\n",
    "        \n",
    "        Arguments:\n",
    "            evaluations: evaluation output from the evaluate_model function\n",
    "            metrics: a list containing one or more of the following metrics:\n",
    "                     'True Positive', 'True Negative', 'False Positive', 'False Negative',\n",
    "                     'Recall', 'Precision', 'Specificity', 'F1 Score', 'IoU', 'TDR'\n",
    "            display_evaluations: one of 'All' to display both overall and class-wise evaluations,\n",
    "                                 'Overall' to display only the overall evaluations,\n",
    "                                 'Class-wise' to display only the classwise evaluations.\n",
    "            class_list: list or tuple containing names of segmentation class.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split evaluations into overall and class-wise evaluations\n",
    "    overall_evaluations = evaluations[\"Overall Evaluations\"]\n",
    "    class_wise_evaluations = evaluations[\"Class-wise Evaluations\"]\n",
    "    \n",
    "    # Validate list of metrics \n",
    "    for metric_id in range(len(metrics)):\n",
    "        metric = metrics[metric_id]\n",
    "        if metric not in overall_evaluations:\n",
    "            raise ValueError(\"'metrics argument' not properly defined. \"\n",
    "                            \"Kindly create a list containing one or more of the following metrics: \"\n",
    "                             \"'True Positive', 'True Negative', 'False Positive', 'False Negative', \"\n",
    "                             \"'Recall', 'Precision', 'Specificity', 'F1 Score', 'IoU', 'TDR'\") \n",
    "    \n",
    "    # Check if class_list is none\n",
    "    if class_list != None and all(isinstance(class_, str) for class_ in class_list):\n",
    "        if len(class_list) == len(class_wise_evaluations[\"Class\"]):\n",
    "            class_list = [class_list]\n",
    "        else:\n",
    "            raise ValueError(\"class_list argument' not properly defined. \" \n",
    "                             \"List is either shorter or longer than segmentation classes\") \n",
    "    else:\n",
    "        class_list = [class_wise_evaluations[\"Class\"]]                             \n",
    "    \n",
    "    # Extract data from the evaluations\n",
    "    overall_data = [overall_evaluations[\"Class\"]] + [overall_evaluations[metrics[metric_id]] for metric_id in range(len(metrics))]\n",
    "    classwise_data = class_list + [class_wise_evaluations[metrics[metric_id]] for metric_id in range(len(metrics))]\n",
    "    overall_data = np.array(overall_data).reshape(1,-1)\n",
    "    classwise_data = np.array(classwise_data).transpose()\n",
    "    \n",
    "    # Determine the type of evaluation report to display\n",
    "    if display_evaluations.lower() == \"all\":\n",
    "        data = np.concatenate((overall_data, classwise_data), axis=0)\n",
    "    elif display_evaluations.lower() == \"overall\":\n",
    "        data = overall_data\n",
    "    elif display_evaluations.lower() == \"class-wise\" or \"classwise\":\n",
    "        data = classwise_data\n",
    "    else:\n",
    "        raise ValueError(\"Display argument are not properly defined.\"\n",
    "                        \"Kindly use 'All' to display both overall and class-wise evaluations.\"\n",
    "                        \"Use 'Overall' to display only the overall evaluations.\"\n",
    "                        \"Or use 'Class-wise' to display only the class-wise evaluations\")\n",
    "\n",
    "    \n",
    "    # Create evaluation report as a pandas dataframe\n",
    "    dataframe = pd.DataFrame(data)\n",
    "    dataframe_titles = [\"Class\"] + metrics\n",
    "    dataframe.columns = dataframe_titles\n",
    "    # dataframe = dataframe.set_index(dataframe_titles[0], col_level=1)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_on_train_dataset = evaluate_model(true_train_masks, predicted_train_masks, n_classes=13)\n",
    "\n",
    "show_evaluations(model_evaluation_on_train_dataset, \n",
    "                 metrics=[\"Recall\", \"Precision\", \"Specificity\", \"IoU\", \"TDR\", \"F1-Score\"], \n",
    "                 class_list=None, \n",
    "                 display_evaluations=\"All\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = true_train_masks\n",
    "y_pred = predicted_train_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of predictions\n",
    "num_classes = y_pred.shape[-1]\n",
    "y_pred = np.array([ np.argmax(y_pred, axis=-1)==i for i in range(num_classes) ]).transpose(1,2,3,0)\n",
    "\n",
    "axes = (1,2) # W,H axes of each image\n",
    "intersection = np.sum(np.logical_and(y_pred, y_true), axis=axes)\n",
    "# intersection = np.sum(np.abs(y_pred * y_true), axis=axes)\n",
    "union = np.sum(np.logical_or(y_pred, y_true), axis=axes)\n",
    "mask_sum = np.sum(np.abs(y_true), axis=axes) + np.sum(np.abs(y_pred), axis=axes)\n",
    "# union = mask_sum  - intersection\n",
    "\n",
    "smooth = .001\n",
    "iou = (intersection + smooth) / (union + smooth)\n",
    "dice = 2 * (intersection + smooth)/(mask_sum + smooth)\n",
    "\n",
    "iou = np.mean(iou)\n",
    "dice = np.mean(dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_train_masks, predicted_train_masks = create_mask(train_generator, model)\n",
    "true_validation_masks, predicted_validation_masks = create_mask(val_generator, model)\n",
    "true_test_masks, predicted_test_masks = create_mask(test_generator, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
